
@misc{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	language = {English},
	urldate = {2019-02-04},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013}
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2018-12-11},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
	file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:/home/kagamino/Zotero/storage/6P4ZNYWY/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf}
}

@misc{lau_learning_nodate,
	title = {Learning {Rate} {Schedules} and {Adaptive} {Learning} {Rate} {Methods} for {Deep} {Learning}},
	url = {https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1},
	language = {English},
	journal = {Towards Data Science},
	author = {Lau, Suki}
}

@misc{sanderson_neural_nodate,
	title = {Neural networks - {Youtube}},
	url = {3b1b.co/neural-networks},
	abstract = {A brief intro to neural networks and backpropagation},
	author = {Sanderson, Grant}
}

@misc{biret_jto-ari-readyplayerone-kick-off-17sep2018_2018,
	title = {{JTO}-{ARI}-{ReadyPlayerOne}-{Kick}-{Off}-17sep2018},
	url = {https://docs.google.com/presentation/d/1JZeYG5q0v4HH7Zm4wSRN4afs9ivyqoerte1E0NPmd8Y},
	urldate = {2018-09-20},
	author = {Biret, Pierre},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/kagamino/Zotero/storage/PJQJG9LW/Biret - 2018 - JTO-ARI-ReadyPlayerOne-Kick-Off-17sep2018.pdf:application/pdf}
}

@inproceedings{tamar_value_2017,
	address = {Melbourne, Australia},
	title = {Value {Iteration} {Networks}},
	isbn = {978-0-9992411-0-3},
	url = {https://www.ijcai.org/proceedings/2017/700},
	doi = {10.24963/ijcai.2017/700},
	abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a ‘planning module’ embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
	language = {en},
	urldate = {2018-09-18},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
	month = aug,
	year = {2017},
	pages = {4949--4953},
	file = {Tamar et al. - 2017 - Value Iteration Networks.pdf:/home/kagamino/Zotero/storage/HBB52VZG/Tamar et al. - 2017 - Value Iteration Networks.pdf:application/pdf}
}

@article{oh_value_nodate,
	title = {Value {Prediction} {Network}},
	abstract = {This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difﬁcult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.},
	language = {en},
	author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
	pages = {11},
	file = {Oh et al. - Value Prediction Network.pdf:/home/kagamino/Zotero/storage/Z92LQ372/Oh et al. - Value Prediction Network.pdf:application/pdf}
}

@article{greydanus_visualizing_nodate,
	title = {Visualizing and {Understanding} {Atari} {Agents}},
	abstract = {While deep reinforcement learning (deep RL) agents are effective at maximizing rewards, it is often unclear what strategies they use to do so. In this paper, we take a step toward explaining deep RL agents through a case study using Atari 2600 environments. In particular, we focus on using saliency maps to understand how an agent learns and executes a policy. We introduce a method for generating useful saliency maps and use it to show 1) what strong agents attend to, 2) whether agents are making decisions for the right or wrong reasons, and 3) how agents evolve during learning. We also test our method on non-expert human subjects and ﬁnd that it improves their ability to reason about these agents. Overall, our results show that saliency information can provide signiﬁcant insight into an RL agent’s decisions and learning behavior.},
	language = {en},
	author = {Greydanus, Sam and Koul, Anurag and Dodge, Jonathan and Fern, Alan},
	pages = {10},
	file = {Greydanus et al. - Visualizing and Understanding Atari Agents.pdf:/home/kagamino/Zotero/storage/A3X9NY4E/Greydanus et al. - Visualizing and Understanding Atari Agents.pdf:application/pdf}
}

@misc{karpathy_convolutional_2015,
	title = {Convolutional {Neural} {Networks} ({CNNs} / {ConvNets})},
	url = {http://cs231n.github.io/convolutional-networks/},
	author = {Karpathy, Andrej},
	year = {2015}
}

@misc{ruder_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://ruder.io/optimizing-gradient-descent/index.html},
	author = {Ruder, Sebastian},
	year = {2016}
}

@incollection{rumelhart_learning_1986,
	address = {Cambridge, MA, USA},
	title = {Learning {Internal} {Representation} by {Error} {Propagation}},
	isbn = {0-262-68053-X},
	url = {http://dl.acm.org/citation.cfm?id=104279.104293},
	booktitle = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}, {Vol}. 1},
	publisher = {MIT Press},
	author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
	editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
	year = {1986},
	pages = {318--362},
	file = {Chap8_PDP86[1].pdf:/home/kagamino/Zotero/storage/RH4BNTF6/Chap8_PDP86[1].pdf:application/pdf}
}

@book{parizeau_reseaux_2004,
	title = {Réseaux de {Neurones}},
	url = {http://wcours.gel.ulaval.ca/2014/h/GIF4101/default/7references/reseauxdeneurones.pdf},
	author = {Parizeau, Marc},
	year = {2004},
	file = {RNF - Reseaux de neurones.pdf:/home/kagamino/Zotero/storage/ANRQMZGF/RNF - Reseaux de neurones.pdf:application/pdf}
}

@inproceedings{lecun_gradient-based_2001,
	title = {Gradient-based learning applied to document recognition},
	booktitle = {Intelligent signal processing},
	publisher = {IEEE Press},
	author = {Lecun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick and Lecun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Pattrick},
	year = {2001},
	pages = {306--351},
	file = {lecun-01a[1].pdf:/home/kagamino/Zotero/storage/38QKJYSQ/lecun-01a[1].pdf:application/pdf}
}

@misc{noauthor_chain_nodate,
	title = {On chain rule, computational graphs, and backpropagation},
	url = {http://outlace.com/Computational-Graph/},
	language = {English},
	journal = {Outlace}
}

@unpublished{goodfellow_deep_2016,
	title = {Deep {Learning}},
	url = {http://www.deeplearningbook.org},
	abstract = {The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. The online version of the book is now complete and will remain available online for free.},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	note = {Book in preparation for MIT Press},
	file = {Deep Learning:/home/kagamino/Zotero/storage/TDWBM78W/www.deeplearningbook.org.html:text/html}
}

@misc{olah_calculus_2015,
	type = {Blog {GitHub}},
	title = {Calculus on {Computational} {Graphs}: {Backpropagation}},
	url = {http://colah.github.io/posts/2015-08-Backprop/},
	language = {English},
	urldate = {2016-10-12},
	journal = {Colas' blog},
	author = {Olah, Christopher},
	month = aug,
	year = {2015},
	file = {Calculus on Computational Graphs\: Backpropagation -- colah's blog:/home/kagamino/Zotero/storage/TNYCCEVL/2015-08-Backprop.html:text/html}
}

@article{bengio_learning_2009,
	title = {Learning {Deep} {Architectures} for {AI}},
	volume = {2},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-006},
	doi = {10.1561/2200000006},
	abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent highlevel abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difﬁcult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
	language = {en},
	number = {1},
	urldate = {2018-09-18},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Bengio, Y.},
	year = {2009},
	pages = {1--127},
	file = {Bengio - 2009 - Learning Deep Architectures for AI.pdf:/home/kagamino/Zotero/storage/VZX2TRVJ/Bengio - 2009 - Learning Deep Architectures for AI.pdf:application/pdf}
}

@article{popescu_multilayer_2009,
	title = {Multilayer {Perceptron} and {Neural} {Networks}},
	volume = {8},
	abstract = {The attempts for solving linear inseparable problems have led to different variations on the number of layers of neurons and activation functions used. The backpropagation algorithm is the most known and used supervised learning algorithm. Also called the generalized delta algorithm because it expands the training way of the adaline network, it is based on minimizing the difference between the desired output and the actual output, through the downward gradient method (the gradient tells us how a function varies in different directions). Training a multilayer perceptron is often quite slow, requiring thousands or tens of thousands of epochs for complex problems. The best known methods to accelerate learning are: the momentum method and applying a variable learning rate. The paper presents the possibility to control the induction driving using neural systems.},
	language = {en},
	number = {7},
	author = {Popescu, Marius-Constantin and Balas, Valentina E and Perescu-Popescu, Liliana and Mastorakis, Nikos},
	year = {2009},
	pages = {10},
	file = {Popescu et al. - 2009 - Multilayer Perceptron and Neural Networks.pdf:/home/kagamino/Zotero/storage/G6PCVNYJ/Popescu et al. - 2009 - Multilayer Perceptron and Neural Networks.pdf:application/pdf}
}

@article{raghu_can_nodate,
	title = {Can {Deep} {Reinforcement} {Learning} {Solve} {Erdos}-{Selfridge}-{Spencer} {Games}?},
	abstract = {Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. These games have a number of appealing features: they are challenging for current learning approaches, but they form (i) a low-dimensional, simply parametrized environment where (ii) there is a linear closed form solution for optimal behavior from any state, and (iii) the difﬁculty of the game can be tuned by changing environment parameters in an interpretable way. We use these Erdos-Selfridge-Spencer games not only to compare different algorithms, but test for generalization, make comparisons to supervised learning, analyze multiagent play, and even develop a self play algorithm.},
	language = {en},
	author = {Raghu, Maithra and Irpan, Alex and Andreas, Jacob and Kleinberg, Robert and Le, Quoc and Kleinberg, Jon},
	pages = {9},
	file = {Raghu et al. - Can Deep Reinforcement Learning Solve Erdos-Selfri.pdf:/home/kagamino/Zotero/storage/S39R3DLE/Raghu et al. - Can Deep Reinforcement Learning Solve Erdos-Selfri.pdf:application/pdf}
}

@article{mnih_human-level_2015-1,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2018-09-18},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
	file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:/home/kagamino/Zotero/storage/MA4ZY2R6/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf}
}